{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Libraries\n",
    "\n",
    "import graphviz\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "\n",
    "from pylab import rcParams\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import scale\n",
    "\n",
    "import seaborn as sb\n",
    "sb.set_style('whitegrid')\n",
    "from collections import Counter\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "import sklearn.metrics as sm\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import xgboost\n",
    "from sklearn.svm import SVR\n",
    "from sklearn import ensemble\n",
    "\n",
    "import math\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import explained_variance_score\n",
    "from sklearn.model_selection import KFold \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import BayesianRidge\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from pyearth import Earth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the Data Set\n",
    "\n",
    "data = pd.read_csv (\"\\\\InputFolderName\\\\Scenario and ANO Data Combined.csv\" )\n",
    "subset3 = data.drop(['Column1','Year', 'gcm.1',\t'Scenario_Year.1',\t'Year.1', 'scenarios',\t'Scenario Number'\t,'Global Outlook'\t,'GCM',\t'Productivity'\t,'Land Use Policy',\t'Hurdle Rate',\t'Constraint Setting',\t'Scenario_Year'\t,'Index_Value', 'capCons',\t'hurdleRate'\t,'climate'], axis= 1)\n",
    "\n",
    "#Process and Prepare the Data\n",
    "\n",
    "#Preapre Data for the Model\n",
    "subset4 = data[['Column1','Year', 'gcm.1',\t'Scenario_Year.1',\t'Year.1', 'scenarios',\t'Scenario Number'\t,'Global Outlook'\t,'GCM',\t'Productivity'\t,'Land Use Policy',\t'Hurdle Rate',\t'Constraint Setting',\t'Scenario_Year'\t,'Index_Value', 'capCons',\t'hurdleRate'\t,'climate']]\n",
    "\n",
    "data4 = data[['Hurdle Rate', 'Constraint Setting']]\n",
    "\n",
    "cleanup_nums = {\n",
    "                \"Hurdle Rate\": {\"5x\": 5 , \"2x\": 2, \"1x\": 1 },\n",
    "                \"Constraint Setting\":{\"U\":0, \"C\":1},\n",
    "                }\n",
    "\n",
    "obj_df2 = data4.replace(cleanup_nums)\n",
    "oe_style = OneHotEncoder()\n",
    "gcm = oe_style.fit_transform(subset4[['gcm.1']])\n",
    "GCM = pd.DataFrame(gcm.toarray(), columns=oe_style.categories_)\n",
    "climate = oe_style.fit_transform(subset4[['climate']])\n",
    "Climate = pd.DataFrame(climate.toarray(), columns=oe_style.categories_)\n",
    "# subset = subset.join(CS)\n",
    "data4 = pd.concat([obj_df2,GCM, Climate], axis=1)\n",
    "data4.columns = ['Hurdle Rate', 'Constraint Setting', 'CE2', 'MPI', 'MR5', 'H3', 'L1', 'M2', 'M3' ]\n",
    "fromatted_data = pd.concat([subset3, data4], axis=1)\n",
    "\n",
    "# the min-max approach (often called normalization) rescales the feature to a fixed range of [0,1] by subtracting the minimum value of the feature and then dividing by the range.\n",
    "# create a scaler object\n",
    "scaler = MinMaxScaler()\n",
    "# fit and transform the data\n",
    "df_norm2 = pd.DataFrame(scaler.fit_transform(fromatted_data), columns=fromatted_data.columns)\n",
    "df_norm2.drop(['bioConstraintMax', 'yieldincreaseB','yieldincreaseS','yieldincreaseD','yieldincreaseI','inputincrease','diselpricepath'], axis='columns', inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Detailed Final Model with all metrices\n",
    "from sklearn import model_selection\n",
    "import HydroErr as he\n",
    "import math\n",
    "def run_models(X_train , y_train, X_test, y_test) :\n",
    "    dfs = []\n",
    "    models = [\n",
    "          ('LinearReg', LinearRegression()),\n",
    "          \n",
    "          ('SVR',SVR(kernel = 'poly', C = 3, degree = 8, gamma =  0.2)),\n",
    "          ('GBRT', ensemble.GradientBoostingRegressor(n_estimators= 500,max_depth= 4, min_samples_split = 5, learning_rate = 0.01,loss= 'ls')),\n",
    "          ('RF', RandomForestRegressor(random_state = 42)),\n",
    "          ('XGB',  xgboost.XGBRegressor(n_estimators=200, learning_rate=0.08, gamma=0, subsample=0.75,\n",
    "                           colsample_bytree=1, max_depth=7)),\n",
    "           ('MARS',Earth() ),\n",
    "           ('BayesianRidge', BayesianRidge(compute_score=True)),\n",
    "           ('KernelRidge', KernelRidge(alpha=1.0))\n",
    "             \n",
    "        ]\n",
    "    results = []\n",
    "    names = []\n",
    "    K=0\n",
    "    scoring = ['r2', 'neg_mean_absolute_error', 'neg_root_mean_squared_error', 'explained_variance']\n",
    "    for name, model in models:\n",
    "        kfold = model_selection.KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "        cv_results = model_selection.cross_validate(model, X_train, y_train, cv=kfold, scoring=scoring, return_train_score=True)\n",
    "        clf = model.fit(X_train, y_train)\n",
    "        y_pred = clf.predict(X_test)\n",
    "        sim=y_pred\n",
    "        obs=y_test\n",
    "        MAE= np.round(he.mae(sim, obs),4)#Mean Absolute Error\n",
    "        MSE= np.round(he.mse(sim, obs),4)#Mean Squared Error\n",
    "        RMSE=np.round(he.rmse(sim, obs),4)#Root Mean Squared Error\n",
    "        MLE=np.round(he.mle(sim, obs),4)#The mean log error value\n",
    "        MALE=np.round(he.male(sim, obs), 4)#The mean absolute log error value.\n",
    "        MSLE=np.round(he.msle(sim, obs), 4)#mean squared log error (MSLE)\n",
    "        MDE=np.round(he.mde(sim, obs),4)#The median error value.\n",
    "        MDAE=np.round(he.mdae(sim, obs),4)#The median absolute error value.\n",
    "        MDSE=np.round(he.mdse(sim, obs),4)#The median squared error value.\n",
    "        ED=np.round(he.ed(sim, obs),4)#The euclidean distance error value.\n",
    "        NED=np.round(he.ned(sim, obs),4)#The normalized euclidean distance value.\n",
    "        RMSLE=np.round(he.rmsle(sim, obs), 4)#The root mean square log error value.\n",
    "\n",
    "        NRMSEV=np.round(he.nrmse_range(sim, obs),4)#The range normalized root mean square error value.\n",
    "        NRMSE=np.round(he.nrmse_mean(sim, obs),4)#mean normalized root mean square error.\n",
    "        MASE=np.round(he.mase(sim, obs),4)#mean absolute scaled error.\n",
    "        r=np.round(he.pearson_r(sim, obs),4)#Pearson correlation coefficient.\n",
    "        r2=np.round(he.r_squared(sim, obs),4)#Coefficient of Determination (r2).\n",
    "        ACC=np.round(he.acc(sim, obs),4)#The anomaly correlation coefficient.\n",
    "        MAPE=np.round(he.mape(sim, obs),4)#The mean absolute percentage error.\n",
    "        MAPD=np.round(he.mapd(sim, obs),4)#The mean absolute percentage deviation.\n",
    "        d=np.round(he.d(sim, obs),4)#Compute the the index of agreement (d).\n",
    "        d1=np.round(he.d1(sim, obs),4)#The index of agreement (d1).\n",
    "        drel=np.round(he.drel(sim, obs),4)#The relative index of agreement.\n",
    "        MBR=np.round(he.mb_r(sim, obs),4)#The Mielke-Berry R value.\n",
    "        NSE=np.round(he.nse(sim, obs),4)#NSE\n",
    "        MNSE=np.round(he.nse_mod(sim, obs),4)#modified Nash-Sutcliffe efficiency value.\n",
    "        RNSE=np.round(he.nse_rel(sim, obs),4)#relative Nash-Sutcliffe efficiency (NSE rel)\n",
    "        KGE1=np.round(he.kge_2009(sim, obs),4)#the Kling-Gupta efficiency (2009).\n",
    "        KGE=np.round(he.kge_2012(sim, obs),4)#Kling-Gupta (2012) efficiency\n",
    "        LM=np.round(he.lm_index(sim, obs),4)#the Legate-McCabe Efficiency Index\n",
    "        LME=np.round(he.d1_p(sim, obs),4)#The Legate-McCabe Efficiency \n",
    "        results.append(cv_results)\n",
    "        names.append(name)\n",
    "        this_df = pd.DataFrame(cv_results)\n",
    "        this_df['model'] = name\n",
    "        this_df['variable'] = y_train.name\n",
    "        this_df['MAE']  = MAE\n",
    "        this_df['RMSE']  = RMSE\n",
    "        this_df['MLE']  = MLE\n",
    "        this_df['MALE']  = MALE\n",
    "        this_df['MSLE']  = MSLE\n",
    "        this_df['MDE']  = MDE\n",
    "        this_df['MDAE']  = MDAE\n",
    "        this_df['MDSE']  = MDSE\n",
    "        this_df['ED']  = ED\n",
    "\n",
    "        this_df['NED']  =  NED\n",
    "        this_df['RMSLE']  = RMSLE\n",
    "        this_df[' NRMSEV']  =  NRMSEV\n",
    "        this_df['NRMSE']  = NRMSE\n",
    "        this_df['MASE']  = MASE\n",
    "\n",
    "        this_df['r']  = r\n",
    "        this_df['r2']  = r2\n",
    "        this_df['ACC']  = ACC\n",
    "        this_df['MAPE']  = MAPE\n",
    "        this_df['MAPD']  = MAPD\n",
    "        this_df['d']  = d\n",
    "        this_df['d1']  = d1\n",
    "        this_df['drel']  = drel\n",
    "\n",
    "        this_df['MBR']  = MBR\n",
    "        this_df['NSE']  = NSE\n",
    "        this_df['MNSE']  = MNSE\n",
    "        this_df['RNSE']  = RNSE\n",
    "        this_df['KGE1']  = KGE1\n",
    "        this_df['KGE']  = KGE\n",
    "        this_df['LM']  = LM\n",
    "        this_df['LME']  = LME\n",
    "\n",
    "        dfs.append(this_df)\n",
    "        final = pd.concat(dfs, ignore_index=True)\n",
    "        \n",
    "    return final.to_csv(\"\\\\OutputFolderName\\\\\"+str(k)+\"_results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in range(0, 70):\n",
    "    data = df_norm2[df_norm2.columns[k]]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(df_norm2.drop(df_norm2.columns[0:70], axis = 1), df_norm2[df_norm2.columns[k]] , train_size=0.8,test_size = 0.2, random_state = 1000)\n",
    "    run_models(X_train, y_train, X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "os.chdir(\"\\\\OutputFolderName\")\n",
    "extension = 'csv'\n",
    "all_filenames = [i for i in glob.glob('*.{}'.format(extension))]\n",
    "#combine all files in the list\n",
    "combined_csv = pd.concat([pd.read_csv(f) for f in all_filenames ])\n",
    "#export to csv\n",
    "combined_csv.to_csv( \"OutputFolderName\\\\Detailed_Model_Run_Results.csv\", index=False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sample upto Year 2050\n",
    "# Latin HyperCube Generated Sample Data (1000 for first 9 variables then rest other variables are used as present of absent)\n",
    "#this uniqu combination gives about 24 combinations thus the sample size becoex 10000*24 = 240000\n",
    "\n",
    "from pyDOE import lhs\n",
    "from scipy.stats.distributions import norm\n",
    "\n",
    "# Visualization\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "# Specify random sample number\n",
    "sample_num = 10000\n",
    "\n",
    "# Using Latin Hypercube method to generate uniform distribution\n",
    "uni_samples = lhs(n=17, samples=sample_num, criterion='maximin')\n",
    "\n",
    "# Create the dataframe\n",
    "df = pd.DataFrame(uni_samples)\n",
    "df.columns = [\n",
    "'yieldincreaseC', \n",
    "'yieldincreaseT', \n",
    "'agprice_path', \n",
    "'isprice_path',\n",
    "'carbonpricepath', \n",
    "'petrolpricepath', \n",
    "'electricitypricepath',\n",
    "'biodivFundCP', \n",
    "'Hurdle Rate', \n",
    "'Constraint Setting', \n",
    "'CE2', \n",
    "'MPI',\n",
    "'MR5', \n",
    "'H3', \n",
    "'L1', \n",
    "'M2', \n",
    "'M3'\n",
    "]\n",
    "\n",
    "df.to_csv(\"\\\\OutputFolder\\\\TestRunData.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training the Best model out of all the models and producing the results based on the Hypercube samples\n",
    "\n",
    "#Testing the model on output SENSITIVITY f0r 2050\n",
    "\n",
    "#ROC Curve for Sensitivity Analaysis of Classification related models.\n",
    "#Importing Required Modules\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_classification\n",
    "from  sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "\n",
    "for k in range(0, 70):\n",
    "    test_data = pd.read_csv (\"\\\\OutputFolder\\\\AllTestData_2050.csv\" )\n",
    "    P_train, P_test, Q_train, Q_test = train_test_split(test_data.drop(test_data.columns[0:1], axis = 1) , test_data[test_data.columns[0]] , train_size=0.99999,test_size = 0.00001, random_state = 45)\n",
    "\n",
    "    #Splitting the data into train and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(df_norm2.drop(df_norm2.columns[0:70], axis = 1), df_norm2[df_norm2.columns[k]] , train_size=0.8,test_size = 0.2, random_state = 500)\n",
    "\n",
    "    #Creating the class object and \n",
    "    model = xgboost.XGBRegressor(n_estimators=200, learning_rate=0.08, gamma=0, subsample=0.75,\n",
    "                               colsample_bytree=1, max_depth=7)\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    #predict probabilities\n",
    "    probs = model.predict(P_train)\n",
    "    df = pd.DataFrame(P_train)\n",
    "    df['Predicted'] = probs\n",
    "    df['Setting'] = test_data[test_data.columns[0]]\n",
    "#     df['Index'] = test_data[test_data.columns[1]]\n",
    "    df['Prediction_For'] = df_norm2.columns[k]\n",
    "    df.to_csv(\"\\\\OutputFolder\\\\SensitivityTest\\\\\"+str(k)+\"_25_03_2022Sensivtivity_results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "os.chdir(\"\\\\OutputFolder\\\\SensitivityTest\")\n",
    "extension = 'csv'\n",
    "all_filenames = [i for i in glob.glob('*.{}'.format(extension))]\n",
    "#combine all files in the list\n",
    "combined_csv = pd.concat([pd.read_csv(f) for f in all_filenames ])\n",
    "#export to csv\n",
    "combined_csv.to_csv( \"\\\\OutputFolder\\\\SensitivityTest\\\\All_Sensitivity_Results.csv\", index=False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "All_SensitivityData = pd.read_csv(\"\\\\OutputFolder\\\\SensitivityTest\\\\SensitivityTest\\\\All_Sensitivity_Results.csv\")\n",
    "Pivot_Test = pd.DataFrame(pd.pivot_table(All_SensitivityData, values='Predicted', index=['Unnamed: 0','Setting'],\n",
    "                    columns=['Prediction_For']))\n",
    "\n",
    "Pivot_Test['Index'] = Pivot_Test.index.to_numpy()\n",
    "\n",
    "# Pivot_Test.to_csv( \"\\\\\\mdsha.homes.deakin.edu.au\\\\my-home\\\\UserData\\\\Desktop\\\\PhD\\\\Year 1\\\\Data\\\\OutPut\\\\Scenario Analysis Test Final Run 17.03.2022\\\\SensitivityTest\\\\TestAll_Sensitivity_Results.csv\", index=False, encoding='utf-8-sig')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
